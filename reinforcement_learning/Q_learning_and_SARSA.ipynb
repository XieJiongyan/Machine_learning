{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Cliff Walking Environment\n",
    "\n",
    "![avatar](fig/cliff_walking.png)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import numpy  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self, length, height):\n",
    "        self.length = length \n",
    "        self.height = height \n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"4 legal actions, 0:up, 1:down, 2:left, 3:right\"\"\"\n",
    "        change = [[0, 1], [0, -1], [-1, 0], [1, 0]]\n",
    "        self.x = min(self.height - 1, max(0, self.x + change[action][0]))\n",
    "        self.y = min(self.length - 1, max(0, self.y + change[action][1]))\n",
    "\n",
    "        states = [self.x, self.y]\n",
    "        reward = -1\n",
    "        terminal = False \n",
    "\n",
    "        if self.x == 0: # if agent is on the cliff line \"SxxxxxT\"\n",
    "            if self.y > 0: # if agent is not on the start position \n",
    "                terminal = True\n",
    "                if self.y != self.length - 1: # if agent falls\n",
    "                    reward = -100\n",
    "\n",
    "        return reward, states, terminal \n",
    "    def reset(self):\n",
    "        self.x = 0\n",
    "        self.y = 0"
   ]
  },
  {
   "source": [
    "## $\\epsilon$-greedy action selection\n",
    "\n",
    "任意$\\forall$选择一个行动时，都有一个概率$\\epsilon \\in [0, 1]$:\n",
    "- $\\epsilon$的概率随机选择一个行动\n",
    "- $1 - \\epsilon$的概率贪心选择最优行动\n",
    "通常可取$\\epsilon = 0.1$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random \n",
    "class Q_table():\n",
    "    def __init__(self, length, height, actions = 4, alpha = 0.1, gamma = 0.9, epsilon = 0.1):\n",
    "        self.table = [0] * actions * length * height \n",
    "        self.actions = actions\n",
    "        self.length = length  \n",
    "        self.height = height \n",
    "        self.alpha = alpha \n",
    "        self.gamma = gamma \n",
    "        self.epsilon = epsilon \n",
    "\n",
    "    def _index(self, a, x, y):\n",
    "        \"\"\"Return the index of Q([x, y], a) in Q_table.\"\"\"\n",
    "        return a * self.height * self.length + x * self.length + y \n",
    " \n",
    "    def take_action(self, x, y, num_episode):\n",
    "        \"\"\"epsilon-greedy action selection\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(4)\n",
    "        else:\n",
    "            actions_value = [self.table[self._index(a, x, y)] for a in range(self.actions)]\n",
    "            return actions_value.index(max(actions_value))\n",
    "\n",
    "    def max_q(self, x, y):\n",
    "        actions_value = [self.table[self._index(a, x, y)] for a in range(self.actions)]\n",
    "        return max(actions_value)\n",
    "\n",
    "    def update(self, a, s0, s1, r, is_terminated):\n",
    "        # both s0, s1 have the form [x,y]\n",
    "        q_predict = self.table[self._index(a, s0[0], s0[1])]\n",
    "        if not is_terminated:\n",
    "            q_target = r + self.gamma * self.max_q(s1[0], s1[1])\n",
    "        else:\n",
    "            q_target = r\n",
    "        self.table[self._index(a, s0[0], s0[1])] += self.alpha * (q_target - q_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cliff_walk():\n",
    "    env = Env(length=12, height=4)\n",
    "    table = Q_table(length=12, height=4)\n",
    "    for num_episode in range(1000):\n",
    "        # within the whole learning process\n",
    "        episodic_reward = 0\n",
    "        is_terminated = False\n",
    "        s0 = [0, 0]\n",
    "        while not is_terminated:\n",
    "            # within one episode\n",
    "            action = table.take_action(s0[0], s0[1], num_episode)\n",
    "            r, s1, is_terminated = env.step(action)\n",
    "            table.update(action, s0, s1, r, is_terminated)\n",
    "            episodic_reward += r\n",
    "            # env.render(frames=100)\n",
    "            s0 = s1\n",
    "        if num_episode % 20 == 0:\n",
    "            print(\"Episode: {}, Score: {}\".format(num_episode, episodic_reward))\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode: 0, Score: -111\nEpisode: 20, Score: -106\nEpisode: 40, Score: -79\nEpisode: 60, Score: -104\nEpisode: 80, Score: -70\nEpisode: 100, Score: -23\nEpisode: 120, Score: -58\nEpisode: 140, Score: -49\nEpisode: 160, Score: -42\nEpisode: 180, Score: -29\nEpisode: 200, Score: -21\nEpisode: 220, Score: -42\nEpisode: 240, Score: -23\nEpisode: 260, Score: -34\nEpisode: 280, Score: -26\nEpisode: 300, Score: -13\nEpisode: 320, Score: -24\nEpisode: 340, Score: -19\nEpisode: 360, Score: -16\nEpisode: 380, Score: -13\nEpisode: 400, Score: -13\nEpisode: 420, Score: -17\nEpisode: 440, Score: -13\nEpisode: 460, Score: -102\nEpisode: 480, Score: -13\nEpisode: 500, Score: -21\nEpisode: 520, Score: -110\nEpisode: 540, Score: -15\nEpisode: 560, Score: -13\nEpisode: 580, Score: -15\nEpisode: 600, Score: -107\nEpisode: 620, Score: -100\nEpisode: 640, Score: -111\nEpisode: 660, Score: -15\nEpisode: 680, Score: -15\nEpisode: 700, Score: -13\nEpisode: 720, Score: -13\nEpisode: 740, Score: -108\nEpisode: 760, Score: -15\nEpisode: 780, Score: -15\nEpisode: 800, Score: -13\nEpisode: 820, Score: -107\nEpisode: 840, Score: -105\nEpisode: 860, Score: -16\nEpisode: 880, Score: -15\nEpisode: 900, Score: -110\nEpisode: 920, Score: -100\nEpisode: 940, Score: -13\nEpisode: 960, Score: -17\nEpisode: 980, Score: -15\n"
     ]
    }
   ],
   "source": [
    "cliff_walk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}