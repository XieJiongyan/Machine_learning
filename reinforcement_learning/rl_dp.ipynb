{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Reinforcement Learning based on dp\n",
    "\n",
    "## Reinforcement Learning \n",
    "\n",
    "There are two method to perform RL:  \n",
    "\n",
    "1. Policy Iteration Learning  \n",
    "2. Value Iteration Learning \n",
    "\n",
    "To perform RL, we must define:  \n",
    "\n",
    "1. Environment  \n",
    "2. ValueTable: maintains valuefunction form state space $S \\in \\mathbb R^2$ to real number space $\\mathbb R$\n",
    "3. Policies: Agent Policies.\n",
    "\n",
    "## Environment \n",
    "\n",
    "### parameter\n",
    "\n",
    "| math | code    | exampvalue | explanation |\n",
    "| ---- | ----    | ----- | -------------------- |\n",
    "| /    | `width`  | 4          | 矩阵宽度     |\n",
    "| /    | `height` | 4          | 矩阵高度     |\n",
    "\n",
    "\n",
    "### Set \n",
    "\n",
    "| math | code                | type  | explanation |\n",
    "| ---- | ----               | ------ | ----------- |\n",
    "| $\\mathcal S$ | `env._state` | (a, b) | 状态集表示迷宫的某一位置        |\n",
    "| $\\mathcal A$ | `MatrixEnv._actions` | \"up\" \"right\" \\.\\.\\. | 处于某状态下的可行动作 |\n",
    "\n",
    "### Variable \n",
    "\n",
    "| math | code | explanation |\n",
    "| ---- | ---- | ----------- |\n",
    "| $s \\in \\mathcal S$  | `s` | state|\n",
    "| $v \\in \\mathbb R$ | `v` | 价值 |\n",
    "| $v_s \\in \\mathbb R$ | `v` | 某状态对应的价值 |\n",
    "| $a \\in \\mathcal A$ | `act` | 可行动作 |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self):\n",
    "        self._states = set()\n",
    "        self._state = None\n",
    "        self._actions = []\n",
    "        self._gamma = None\n",
    "        \n",
    "    @property\n",
    "    def states(self):\n",
    "        return self._states\n",
    "    \n",
    "    @property\n",
    "    def state_space(self):\n",
    "        return self._state_shape\n",
    "    \n",
    "    @property\n",
    "    def actions(self):\n",
    "        return self._actions\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return len(self._actions)\n",
    "    \n",
    "    @property\n",
    "    def gamma(self):\n",
    "        return self._gamma\n",
    "    \n",
    "    def _world_init(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def reset(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        \"\"\"Return distribution and next states\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def set_state(self, state):\n",
    "        self._state = state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixEnv(Env):\n",
    "    def __init__(self, height=4, width=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._action_space = 4\n",
    "        self._actions = [\"up\", \"right\", \"down\", \"left\"]\n",
    "        \n",
    "        self._state_shape = (2,)\n",
    "        self._state_shape = (height, width)\n",
    "        self._states = [(i, j) for i in range(height) for j in range(width)]\n",
    "        \n",
    "        self._gamma = 0.9\n",
    "        self._height = height\n",
    "        self._width = width\n",
    "\n",
    "        self._world_init()\n",
    "        \n",
    "    @property\n",
    "    def state(self):\n",
    "        return self._state\n",
    "    \n",
    "    @property\n",
    "    def gamma(self):\n",
    "        return self._gamma\n",
    "    \n",
    "    def set_gamma(self, value):\n",
    "        self._gamma = value\n",
    "        \n",
    "    def reset(self):\n",
    "        self._state = self._start_point\n",
    "        \n",
    "    def _world_init(self):\n",
    "        # start_point\n",
    "        self._start_point = (0, 0)\n",
    "        self._end_point = (self._height - 1, self._width - 1)\n",
    "        \n",
    "    def _state_switch(self, act):\n",
    "        # 0: h - 1, 1: w + 1, 2: h + 1, 3: w - 1\n",
    "        if act == \"up\":  # up\n",
    "            self._state = (max(0, self._state[0] - 1), self._state[1])\n",
    "        elif act == \"right\":  # right\n",
    "            self._state = (self._state[0], min(self._width - 1, self._state[1] + 1))\n",
    "        elif act == \"down\":  # down\n",
    "            self._state = (min(self._height - 1, self._state[0] + 1), self._state[1])\n",
    "        elif act == \"left\":  # left\n",
    "            self._state = (self._state[0], max(0, self._state[1] - 1))\n",
    "\n",
    "    def step(self, act):\n",
    "        assert act in self._actions\n",
    "        \n",
    "        done = False\n",
    "        reward = 0.\n",
    "\n",
    "        self._state_switch(act)\n",
    "        \n",
    "        if self._state == self._end_point:\n",
    "            reward = 1.\n",
    "            done = True\n",
    "\n",
    "        return None, done, [reward], [self._state]"
   ]
  },
  {
   "source": [
    "## ValueTable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueTable:\n",
    "    def __init__(self, env):\n",
    "        self._values = np.zeros(env.state_space)\n",
    "        \n",
    "    def update(self, s, value):\n",
    "        self._values[s] = value\n",
    "        \n",
    "    def get(self, state):\n",
    "        if type(state) == tuple:\n",
    "            return self._values[state]\n",
    "        if type(state) == list:\n",
    "            # loop get\n",
    "            res = [self._values[s] for s in state]\n",
    "            return res\n",
    "        elif type(state) == tuple:\n",
    "            # return directly\n",
    "            return self._values[state]"
   ]
  },
  {
   "source": [
    "## Policies\n",
    "对于任何一个状态$s$，都拥有一个策略$\\pi$，对于每个状态的每一个策略，它无非是通过某个动作$a$选择迁移到可以迁移的任一状态$s'$，并且拥有不同的迁移概率$\\pi_{sa}$。\n",
    "\n",
    "### variable\n",
    "\n",
    "|math| code| explanation |\n",
    "| --- | --- | ---------- | \n",
    "| $\\pi_s \\in \\vec {\\mathbb R} $ | `Policies._policies[state]` | 在某个状态下的策略，包括可以转移到状态和与之对应的转移概率 |\n",
    "| $\\pi_{sa} \\in [0, 1]$ | `Policies._policies[state].prob[... Policies._policies[state].act.index(act)]` | 状态$s$进行动作$a$的概率 |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "Pi = namedtuple('Pi', ['act', 'prob'])\n",
    "\n",
    "\n",
    "class Policies:\n",
    "    def __init__(self, env: Env):\n",
    "        self._actions = env.actions\n",
    "        self._default_policy = [1 / env.action_space] * env.action_space\n",
    "        self._policies = dict.fromkeys(env.states, Pi(self._actions, self._default_policy))\n",
    "        # print(\"Policies:\", self._policies)\n",
    "        # print(self._policies[(1, 0)].prob)\n",
    "    \n",
    "    def sample(self, state):\n",
    "        if self._policies.get(state, None) is None:\n",
    "            self._policies[state] = Pi(self._actions, self._default_policy)\n",
    "\n",
    "        policy = self._policies[state]\n",
    "        return np.random.choice(policy.act, p=policy.prob)\n",
    "    \n",
    "    def retrieve(self, state):\n",
    "        return self._policies[state].prob\n",
    "    \n",
    "    def update(self, state, policy):\n",
    "        self._policies[state] = self._policies[state]._replace(prob=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pi(act=['up', 'down'], prob=[0.15, 0.25])\n0.15\n"
     ]
    }
   ],
   "source": [
    "Pi = namedtuple('Pi', ['act', 'prob'])\n",
    "pi = Pi([\"up\", \"down\"], [0.15, 0.25])\n",
    "print(pi)\n",
    "print(pi.prob[pi.act.index(\"up\")])"
   ]
  },
  {
   "source": [
    "## Policy Iteration Learning\n",
    "### Parameter\n",
    "| math | code       | explanation|\n",
    "| ---- |   ----     | ---------- |\n",
    "|      | `iteration`  | 循环次数    |\n",
    "| /    | `upper_bound` |  /          |\n",
    "### Pesudo\n",
    "1. initialization\n",
    "    $V(s) \\in R$ and $\\pi(s) \\in \\mathcal A(s) \\quad \\forall s \\in \\mathcal S$\n",
    "    其中"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "\n",
    "def policy_eval(env, values, policies, upper_bound):\n",
    "    print('\\n===== Policy Evalution =====')\n",
    "    delta = upper_bound\n",
    "    iteration = 0\n",
    "\n",
    "    while delta >= upper_bound:\n",
    "        delta = 0.\n",
    "\n",
    "        for s in env.states:\n",
    "            v = values.get(s)\n",
    "            env.set_state(s)\n",
    "\n",
    "            action_index = policies.sample(s)\n",
    "            # print(action_index)\n",
    "            # print(env.actions)\n",
    "            action = policies.sample(s)\n",
    "            \n",
    "            _, _, rewards, next_states = env.step(action)\n",
    "\n",
    "            next_values = values.get(list(next_states))\n",
    "            td_values = list(map(lambda x, y: x + env.gamma * y, rewards, next_values))\n",
    "\n",
    "            exp_value = np.mean(td_values)\n",
    "            values.update(s, exp_value)\n",
    "\n",
    "            # update delta\n",
    "            delta = max(delta, abs(v - exp_value))\n",
    "            \n",
    "        iteration += 1\n",
    "        print('\\r> iteration: {} delta: {}'.format(iteration, delta), flush=True, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improve(env, values, policies):\n",
    "    print('\\n===== Policy Improve =====')\n",
    "    policy_stable = True\n",
    "    \n",
    "    for state in env.states:\n",
    "        old_act = policies.sample(state)\n",
    "\n",
    "        # calculate new policy execution\n",
    "        actions = env.actions\n",
    "        value = [0] * len(env.actions)\n",
    "        \n",
    "        for i, action in enumerate(actions):\n",
    "            env.set_state(state)\n",
    "            _, _, rewards, next_states = env.step(action)\n",
    "            next_values = values.get(list(next_states))\n",
    "            td_values = list(map(lambda x, y: x + env.gamma * y, rewards, next_values))\n",
    "            prob = [1 / len(next_states)] * len(next_states)\n",
    "\n",
    "            value[i] = sum(map(lambda x, y: x * y, prob, td_values))\n",
    "\n",
    "        # action selection\n",
    "        new_act = env._actions[np.argmax(value)]\n",
    "        # print(value)\n",
    "        # print(\"new_act:\", new_act, old_act)\n",
    "\n",
    "        # greedy update policy\n",
    "        # print(env.action_space)\n",
    "        new_policy = [0.] * env.action_space\n",
    "        new_policy[env._actions.index(new_act)] = 1.\n",
    "        policies.update(state, new_policy)\n",
    "\n",
    "        if old_act != new_act:\n",
    "            policy_stable = False\n",
    "\n",
    "    return policy_stable"
   ]
  },
  {
   "source": [
    "## Solving Matrix Game via Policy Iteration Learning \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 2 delta: 0.8999999999999999\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 2 delta: 0.7290000000000001\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 2 delta: 0.5904900000000004\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 2 delta: 0.47829690000000014\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 2 delta: 0.38742048900000015\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 2 delta: 0.3138105960900006\n",
      "===== Policy Improve =====\n",
      "\n",
      "===== Policy Evalution =====\n",
      "> iteration: 2 delta: 0.25418658283290085\n",
      "===== Policy Improve =====\n",
      "\n",
      "[time consumpution]: 0.05784320831298828 s\n",
      "Evaluation: [reward] 1.0 [step] 6\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env = MatrixEnv(width=4, height=4)  # TODO(ming): try different word size\n",
    "policies = Policies(env)\n",
    "values = ValueTable(env)\n",
    "upper_bound = 1\n",
    "\n",
    "stable = False\n",
    "\n",
    "start = time.time()\n",
    "while not stable:\n",
    "    policy_eval(env, values, policies, upper_bound)\n",
    "    stable = policy_improve(env, values, policies)\n",
    "end = time.time()\n",
    "\n",
    "print('\\n[time consumpution]: {} s'.format(end - start))\n",
    "\n",
    "done = False\n",
    "rewards = 0\n",
    "env.reset()\n",
    "step = 0\n",
    "\n",
    "while not done:\n",
    "    act_index = policies.sample(env.state)\n",
    "    # print(act_index)\n",
    "    _, done, reward, next_state = env.step(env.actions[env._actions.index(act_index)])\n",
    "    rewards += sum(reward)\n",
    "    step += 1\n",
    "\n",
    "print('Evaluation: [reward] {} [step] {}'.format(rewards, step))"
   ]
  },
  {
   "source": [
    "## value Iteration\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iter(env, values, upper_bound):\n",
    "    print('===== Value Iteration =====')\n",
    "    delta = upper_bound + 1.\n",
    "    states = copy(env.states)\n",
    "    \n",
    "    iteration = 0\n",
    "\n",
    "    while delta >= upper_bound:\n",
    "        delta = 0\n",
    "\n",
    "        for s in states:\n",
    "            v = values.get(s)\n",
    "\n",
    "            # get new value\n",
    "            actions = env.actions\n",
    "            vs = [0] * len(actions)\n",
    "\n",
    "            for i, action in enumerate(actions):\n",
    "                env.set_state(s)\n",
    "                _, _, rewards, next_states = env.step(action)\n",
    "                td_values = list(map(lambda x, y: x + env.gamma * y, rewards, values.get(next_states)))\n",
    "\n",
    "                vs[i] = np.mean(td_values)\n",
    "\n",
    "            values.update(s, max(vs))\n",
    "            delta = max(delta, abs(v - values.get(s)))\n",
    "        \n",
    "        iteration += 1\n",
    "        print('\\r> iteration: {} delta: {}'.format(iteration, delta), end=\"\", flush=True)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "===== Value Iteration =====\n",
      "> iteration: 89 delta: 9.404610870067387e-05\n",
      "===== Policy Improve =====\n",
      "\n",
      "[time consumption] 0.5226085186004639s\n",
      "Evaluation: [reward] 1.0 [step] 14\n"
     ]
    }
   ],
   "source": [
    "env = MatrixEnv(width=8, height=8)  # try different word size\n",
    "policies = Policies(env)\n",
    "values = ValueTable(env)\n",
    "upper_bound = 1e-4\n",
    "\n",
    "start = time.time()\n",
    "value_iter(env, values, upper_bound)\n",
    "_ = policy_improve(env, values, policies)\n",
    "end = time.time()\n",
    "\n",
    "print('\\n[time consumption] {}s'.format(end - start))\n",
    "# print(\"===== Render =====\")\n",
    "env.reset()\n",
    "done = False\n",
    "rewards = 0\n",
    "step = 0\n",
    "while not done:\n",
    "    act_index = policies.sample(env.state)\n",
    "    # print(act_index)\n",
    "    _, done, reward, next_state = env.step(act_index)\n",
    "    rewards += sum(reward)\n",
    "    step += 1\n",
    "\n",
    "print('Evaluation: [reward] {} [step] {}'.format(rewards, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}